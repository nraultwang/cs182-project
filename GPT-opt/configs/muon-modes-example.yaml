# Configuration for testing different Muon parameter grouping modes
# GPT-2 Small (124M params) on FineWeb1B dataset

# Three modes for applying Muon optimizer:
# 1. stacked_qkv: Muon on stacked QKV matrices + rest of parameters (default, current behavior)
# 2. split_qkv: Muon on split Q/K/V matrices + rest of parameters  
# 3. voh_only: Muon on split heads for V, W_O, FFN only (AdamW on Q, K, and rest)

# Optimizer settings
optimizer_params:
  # MODE 1: Stacked QKV (default, current behavior)
  # Applies Muon to the full stacked QKV weight matrix
  - name: muon-polarexpress
    lr: [0.01]
    weight_decay: 0.1
    momentum: 0.95
    ns_steps: 5
    rms_scaling: true
    lr_schedule: constant-linear
    warm_up_fraction: 0.4
    polar_num_iters: 5
    polar_safety: 1.01
    polar_cushion: 0.024
    muon_mode: stacked_qkv  # Default mode

  # MODE 2: Split QKV
  # Splits QKV into separate Q, K, V matrices and applies Muon to each
  # Also splits by attention heads for finer-grained updates
  - name: muon-polarexpress
    lr: [0.01]
    weight_decay: 0.1
    momentum: 0.95
    ns_steps: 5
    rms_scaling: true
    lr_schedule: constant-linear
    warm_up_fraction: 0.4
    polar_num_iters: 5
    polar_safety: 1.01
    polar_cushion: 0.024
    muon_mode: split_qkv  # Split Q, K, V separately

  # MODE 3: V, O, H only (VOH)
  # Applies Muon only to:
  # - V (Value) projection matrices (split by heads)
  # - W_O (Output) projection matrices
  # - FFN (Feed-Forward Network) matrices
  # Uses AdamW for Q and K projections and other parameters
  - name: muon-polarexpress
    lr: [0.01]
    weight_decay: 0.1
    momentum: 0.95
    ns_steps: 5
    rms_scaling: true
    lr_schedule: constant-linear
    warm_up_fraction: 0.4
    polar_num_iters: 5
    polar_safety: 1.01
    polar_cushion: 0.024
    muon_mode: voh_only  # Muon on V, W_O, FFN only (AdamW on Q, K)

# Training parameters
training_params:
  tokens_processed: 524288      # 2^19
  val_tokens_processed: 8388608 # 2^23
  batch_size: 16
  num_epochs: 1
  context_length: 1024
  gradnorm: 1.0
  tensorcore_precision: high
  autocast: true
  mixed_precision: bfloat16
  compile: true

# Logging parameters
logging_params:
  val_tokens_processed: 8388608
  log_step: 50
  val_step: 500
  save_ckpt_step: 500
  load_ckpt_step: 0
  keep_last: 2
  ckpt_dir: "outputs/checkpoints"
  results_dir: "outputs/results"
  wandb:
    project: "muon-modes-comparison"
    dir: "outputs/wandb"

# GPT-2 Small: 124M params
gpt_model:
  n_embd: 768
  n_layer: 12
  n_head: 12
  vocab_size: 50257
  flash_attention: true

# FineWeb1B dataset
dataset:
  name: "fineweb1B"
