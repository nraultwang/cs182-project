program: run_hydra.py
method: grid
metric:
  name: val/loss
  goal: minimize

# Tell wandb to use Hydra override syntax (no -- prefix)
command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}

parameters:
  # Base configuration
  hydra.job.name:
    value: "phase2-adamw"

  # Dataset and model (same as Phase 2 Muon runs)
  training_data:
    value: fineweb1B

  gpt_model:
    value: gpt-small

  # AdamW optimizer (Phase 2 baseline)
  optimizer_params.name:
    value: adamw

  # Learning rates: powers-of-two multiples of the Phase 0 best LR (0.0003)
  #   1x  → 0.0003
  #   2x  → 0.0006
  #   4x  → 0.0012
  optimizer_params.args.lr:
    values: [0.0003, 0.0012, 0.0048, 0.0192, 0.0768]

  optimizer_params.args.weight_decay:
    value: 0.01

  # AdamW betas from the baseline config
  optimizer_params.args.betas:
    value: [0.9, 0.999]

  # Seeds for Phase 2 (match Muon Phase 2 runs)
  seed:
    values: [42, 123, 456]

  # Training parameters (0.3 epochs final evaluation)
  training_data.training_params.num_epochs:
    value: 0.3
 
  training_data.training_params.batch_size:
    value: 32

  training_data.training_params.tokens_processed:
    value: 524288

  # Logging (same cadence as Phase 2 Muon stage)
  # Values are in micro-steps: 160=10 opt steps, 800=50 opt steps, 3200=200 opt steps
  logging_params.log_step:
    value: 160

  logging_params.diag_log_step:
    value: 800

  logging_params.val_step:
    value: 3200

  logging_params.svd_log_step:
    value: 800

  logging_params.save_ckpt_step:
    value: 3200

  logging_params.wandb.project:
    value: "cs182-project-GPT-opt"

  logging_params.wandb.tags:
    value: ["phase2", "adamw", "baseline-grid"]

# Phase 2 AdamW sweep:
#   lr ∈ {0.0003, 0.0006, 0.0012} (powers-of-two multiples of 0.0003)
#   seed ∈ {42, 123, 456}
# → 3 × 3 = 9 AdamW runs, mirroring Phase 2 Muon seeds and training schedule.
