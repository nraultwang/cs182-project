program: run_hydra.py
method: grid
metric:
  name: val/loss
  goal: minimize

# Tell wandb to use Hydra override syntax (no -- prefix)
command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}

parameters:
  # Base configuration
  hydra.job.name:
    value: "phase0-adamw"
  
  # Dataset and model
  training_data:
    value: fineweb1B
  
  gpt_model:
    value: gpt-small
  
  # AdamW optimizer
  optimizer_params.name:
    value: adamw
  
  # AdamW learning rates (typical range for GPT-2: 1e-4 to 6e-4)
  optimizer_params.args.lr:
    values: [0.00001, 0.0001, 0.0003, 0.0006, 0.001]
  
  optimizer_params.args.weight_decay:
    values: [0.01, 0.1]
  
  # AdamW betas (momentum coefficients)
  optimizer_params.args.betas:
    values: [[0.9, 0.999], [0.95, 0.999], [0.95, 0.95]]
  
  # Shorter training for baseline validation (0.2 epochs for fast LR search)
  training_data.training_params.num_epochs:
    value: 0.2
  
  training_data.training_params.tokens_processed:
    value: 524288
  
  # Logging (adjusted for gradient accumulation=16)
  # Values are in micro-steps: 160=10 opt steps, 800=50 opt steps, 1600=100 opt steps
  logging_params.log_step:
    value: 160
  
  logging_params.diag_log_step:
    value: 160
  
  logging_params.val_step:
    value: 8000
  
  logging_params.svd_log_step:
    value: 800
  
  logging_params.save_ckpt_step:
    value: 32000
  
  logging_params.wandb.project:
    value: "cs182-polar-express"
  
  logging_params.wandb.tags:
    value: ["phase0", "baseline", "adamw"]

# This creates: 3 LRs × 2 weight_decays × 3 betas = 18 AdamW runs
