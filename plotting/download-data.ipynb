{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:35:39.543984Z",
     "start_time": "2025-12-04T07:35:38.675284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "id": "b45bc1f03cdc6844",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-04T07:35:39.554333Z",
     "start_time": "2025-12-04T07:35:39.548594Z"
    }
   },
   "source": [
    "# --- HELPER FUNCTION ---\n",
    "def process_single_run(run):\n",
    "    try:\n",
    "        # Skip if not finished to save time\n",
    "        if run.state != \"finished\":\n",
    "            return None\n",
    "\n",
    "        # 1. Extract Config\n",
    "        try:\n",
    "            opt_args = run.config.get(\"optimizer_params\", {}).get(\"args\", {})\n",
    "            safety_factor = opt_args.get(\"polar_safety\")\n",
    "        except Exception:\n",
    "            safety_factor = None\n",
    "\n",
    "        # 2. Download History\n",
    "        # scan_history handles pagination to prevent 500 Errors\n",
    "        history_generator = run.scan_history()\n",
    "        history_list = [row for row in history_generator]\n",
    "\n",
    "        if not history_list:\n",
    "            return None\n",
    "\n",
    "        # 3. Build DataFrame\n",
    "        df = pd.DataFrame(history_list)\n",
    "\n",
    "        # 4. Add Metadata\n",
    "        df[\"run_id\"] = run.id\n",
    "        df[\"run_name\"] = run.name\n",
    "        df[\"sweep_id\"] = run.sweep.id if run.sweep else \"no_sweep\"\n",
    "        df[\"safety_factor\"] = safety_factor\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# --- 1. ROBUST LOADER ---\n",
    "def load_data(filepath=\"./data/master_project_data.csv\"):\n",
    "    \"\"\"\n",
    "    Loads the CSV with low_memory=False to handle mixed types\n",
    "    (fixing the DtypeWarning) and parses known numeric columns.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {filepath}...\")\n",
    "    # low_memory=False forces pandas to read the whole file to determine dtypes\n",
    "    # instead of guessing chunk-by-chunk.\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    print(f\"Loaded {len(df)} rows. Columns found: {len(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "# --- 2. FAST PLOTTER ---\n",
    "def plot_fast(df, x_col, y_col, hue_col=None, title=None, log_scale=False):\n",
    "    \"\"\"\n",
    "    Wrapper for Seaborn lineplot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x=x_col,\n",
    "        y=y_col,\n",
    "        hue=hue_col,\n",
    "        errorbar=\"sd\" # Shows Standard Deviation shadow\n",
    "    )\n",
    "\n",
    "    if log_scale:\n",
    "        plt.yscale(\"log\")\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    else:\n",
    "        plt.title(f\"{y_col} vs {x_col}\")\n",
    "\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# --- 3. THE DEBUGGER (Why your plot failed) ---\n",
    "def find_col(df, keyword):\n",
    "    \"\"\"Prints all columns containing the keyword (e.g., 'loss')\"\"\"\n",
    "    matches = [c for c in df.columns if keyword in str(c).lower()]\n",
    "    print(f\"Columns matching '{keyword}':\")\n",
    "    print(matches)\n",
    "\n",
    "\n",
    "# --- HELPER: FLATTEN CONFIG ---\n",
    "def flatten_config(config):\n",
    "    \"\"\"Flattens nested W&B config dictionary.\"\"\"\n",
    "    out = {}\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            for sub_key, sub_value in value.items():\n",
    "                out[f\"config_{key}.{sub_key}\"] = sub_value\n",
    "        else:\n",
    "            out[f\"config_{key}\"] = value\n",
    "    return out\n",
    "\n",
    "# --- HELPER: PROCESS SINGLE RUN METADATA ---\n",
    "def process_run_metadata(run):\n",
    "    \"\"\"Extracts metadata for the manifest. Fast operation.\"\"\"\n",
    "    try:\n",
    "        if run.state != \"finished\":\n",
    "            return None\n",
    "\n",
    "        # Get Sweep ID safely\n",
    "        sweep_id = run.sweep.id if run.sweep else \"manual_runs\"\n",
    "\n",
    "        # Build Manifest Row\n",
    "        meta = {\n",
    "            \"run_id\": run.id,\n",
    "            \"run_name\": run.name,\n",
    "            \"sweep_id\": sweep_id,\n",
    "            \"state\": run.state,\n",
    "            \"created_at\": run.created_at\n",
    "        }\n",
    "\n",
    "        # Merge config\n",
    "        meta.update(flatten_config(run.config))\n",
    "\n",
    "        # Return tuple: (sweep_id, run_object, meta_dict)\n",
    "        # We return the run object so we can group them later without re-fetching\n",
    "        return (sweep_id, run, meta)\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# --- HELPER: DOWNLOAD HISTORY (From previous step) ---\n",
    "def download_run_history(run):\n",
    "    try:\n",
    "        history_gen = run.scan_history()\n",
    "        history_list = [row for row in history_gen]\n",
    "        if not history_list:\n",
    "            return None\n",
    "        df = pd.DataFrame(history_list)\n",
    "        df[\"run_id\"] = run.id\n",
    "        return df\n",
    "    except:\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:35:39.560963Z",
     "start_time": "2025-12-04T07:35:39.559182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- CONFIGURATION ---\n",
    "ENTITY = \"justin_yang-university-of-california-berkeley\"\n",
    "PROJECT = \"cs182-project-GPT-opt\"\n",
    "DATA_DIR = \"./data\"\n",
    "SWEEPS_DIR = os.path.join(DATA_DIR, \"sweeps\")\n",
    "MANIFEST_FILE = os.path.join(DATA_DIR, \"runs_manifest.csv\")\n",
    "MAX_WORKERS = 64  # Download 64 runs at once (Safe for W&B API limits)"
   ],
   "id": "633ca75f1a13d0b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:36:23.818502Z",
     "start_time": "2025-12-04T07:35:39.566540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Setup\n",
    "os.makedirs(SWEEPS_DIR, exist_ok=True)\n",
    "api = wandb.Api(timeout=60)\n",
    "\n",
    "print(f\"Fetching run list from {ENTITY}/{PROJECT}...\")\n",
    "# This just gets the iterator, doesn't download details yet\n",
    "runs = api.runs(f\"{ENTITY}/{PROJECT}\")\n",
    "\n",
    "# 2. Parallel Manifest Build\n",
    "print(f\"Building Manifest with {MAX_WORKERS} workers...\")\n",
    "\n",
    "manifest_data = []\n",
    "runs_by_sweep = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all runs to the pool\n",
    "    # Note: We cast runs to list to force the initial pagination fetch\n",
    "    futures = [executor.submit(process_run_metadata, run) for run in list(runs)]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Metadata\"):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            s_id, run_obj, meta_dict = result\n",
    "\n",
    "            # Add to Manifest list\n",
    "            manifest_data.append(meta_dict)\n",
    "\n",
    "            # Group for the next step (Download History)\n",
    "            if s_id not in runs_by_sweep:\n",
    "                runs_by_sweep[s_id] = []\n",
    "            runs_by_sweep[s_id].append(run_obj)\n",
    "\n",
    "# Save Manifest\n",
    "if manifest_data:\n",
    "    manifest_df = pd.DataFrame(manifest_data)\n",
    "    manifest_df.to_csv(MANIFEST_FILE, index=False)\n",
    "    print(f\"Manifest saved to {MANIFEST_FILE} ({len(manifest_df)} runs)\")\n",
    "else:\n",
    "    print(\"No finished runs found.\")"
   ],
   "id": "825f1a23ce8722c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching run list from justin_yang-university-of-california-berkeley/cs182-project-GPT-opt...\n",
      "Building Manifest with 64 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Metadata: 100%|██████████| 491/491 [00:01<00:00, 277.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest saved to ./data/runs_manifest.csv (441 runs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:37:30.809820Z",
     "start_time": "2025-12-04T07:36:29.931321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. GLOBAL PARALLEL DOWNLOAD (Optimized)\n",
    "print(f\"Starting Global Download for {len(runs)} total runs...\")\n",
    "\n",
    "# Storage for the results: {run_id: dataframe}\n",
    "# We keep this in memory because 300MB is trivial for modern RAM\n",
    "run_data_map = {}\n",
    "\n",
    "# Filter only finished runs to avoid wasting time on crashed ones\n",
    "valid_runs = [r for r in runs if r.state == \"finished\"]\n",
    "\n",
    "# Define the worker function inside or outside (helper defined previously)\n",
    "# def download_run_history(run): ... (Keep your helper function)\n",
    "\n",
    "# Execute ALL downloads in one massive pool\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit every single run to the pool at once\n",
    "    future_to_run = {executor.submit(download_run_history, run): run for run in valid_runs}\n",
    "\n",
    "    # Process results as they arrive (Order doesn't matter here)\n",
    "    for future in tqdm(as_completed(future_to_run), total=len(valid_runs), desc=\"Downloading History\"):\n",
    "        run = future_to_run[future]\n",
    "        try:\n",
    "            df = future.result()\n",
    "            if df is not None and not df.empty:\n",
    "                run_data_map[run.id] = df\n",
    "        except Exception as e:\n",
    "            # Print error but keep going\n",
    "            print(f\"Warning: Failed to download run {run.id}: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. GROUP AND SAVE (The \"Reduce\" Step)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Grouping data by sweep and saving to disk...\")\n",
    "\n",
    "# Now we iterate through our sweep grouping to save the files\n",
    "for sweep_id, sweep_runs in tqdm(runs_by_sweep.items(), desc=\"Saving Files\"):\n",
    "\n",
    "    sweep_dfs = []\n",
    "    for run in sweep_runs:\n",
    "        # Check if we successfully downloaded data for this run\n",
    "        if run.id in run_data_map:\n",
    "            sweep_dfs.append(run_data_map[run.id])\n",
    "\n",
    "    # Only save if we have data for this sweep\n",
    "    if sweep_dfs:\n",
    "        sweep_final_df = pd.concat(sweep_dfs, ignore_index=True, sort=False)\n",
    "\n",
    "        # Save using the Sweep ID\n",
    "        output_path = os.path.join(SWEEPS_DIR, f\"sweep_{sweep_id}.csv\")\n",
    "        sweep_final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"DONE! All sweep files created.\")"
   ],
   "id": "1b3a49ce0ecfa064",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Global Download for 491 total runs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading History: 100%|██████████| 441/441 [00:51<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping data by sweep and saving to disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Files: 100%|██████████| 28/28 [00:09<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE! All sweep files created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7b91037d29b2b948"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
