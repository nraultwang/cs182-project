{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7158198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d28406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# W&B project info\n",
    "ENTITY = \"justin_yang-university-of-california-berkeley\"\n",
    "PROJECT = \"cs182-project-GPT-opt\"\n",
    "\n",
    "# Local cache directory for plotting data (ignored by git)\n",
    "PLOTTING_CACHE_DIR = Path(\"plotting\") / \"cache\"\n",
    "PLOTTING_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Map high-level optimizer categories to sweep IDs or run IDs.\n",
    "ATTN_CATEGORIES = {\n",
    "    \"AdamW\": [\"pxrm9xt8\", \"hkw05gba\", \"eifb7cyl\", \"4uytpec5\"],\n",
    "    \"Muon-NS-All\": [\"qmzevjll\", \"lh2tzeh1\", \"capygls4\"],\n",
    "    \"Muon-NS-VOFFN\": [\"88vr9lps\"],\n",
    "    \"Muon-PE-All\": [\"emzbw2qc\", \"q02rln6i\"],\n",
    "    \"Muon-PE-VOFFN\": [\"67g7mm51\"],\n",
    "}\n",
    "\n",
    "# Titles for the attention entropy grid\n",
    "ATTN_CATEGORY_TITLES = {\n",
    "    \"AdamW\": \"AdamW\",\n",
    "    \"Muon-NS-All\": \"Muon Newton-Schulz (QKV, O, FFN)\",\n",
    "    \"Muon-NS-VOFFN\": \"Muon Newton-Schulz (V, O, FFN)\",\n",
    "    \"Muon-PE-All\": \"Muon Polar Express (QKV, O, FFN)\",\n",
    "    \"Muon-PE-VOFFN\": \"Muon Polar Express (V, O, FFN)\",\n",
    "}\n",
    "\n",
    "# Plot layout knobs for the combined attention grid\n",
    "DEFAULT_YLIM = (0.0, 6.0)\n",
    "USE_EPOCH_X = True\n",
    "EPOCH_MAX = 0.3  # runs are 0.3 epochs\n",
    "\n",
    "COMBINED_LEGEND_Y = 1.08\n",
    "COMBINED_SUBTITLE_Y = 0.81\n",
    "COMBINED_TITLE_Y = 0.87\n",
    "COMBINED_TIGHT_RECT = (0, 0, 1, 0.86)\n",
    "COMBINED_SUBTITLE_X_NUDGE = {\n",
    "    \"AdamW\": -0.07,\n",
    "    \"Muon-NS-All\": -0.03,\n",
    "    \"Muon-PE-All\": 0,\n",
    "    \"Muon-NS-VOFFN\": 0.03,\n",
    "    \"Muon-PE-VOFFN\": 0.07,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253949e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _collect_runs_for_ids(ids, entity=ENTITY, project=PROJECT):\n",
    "    \"\"\"Given a list of sweep IDs and/or run IDs, return a flat list of runs.\"\"\"\n",
    "    api = wandb.Api()\n",
    "    runs = []\n",
    "    for _id in ids:\n",
    "        try:\n",
    "            sweep = api.sweep(f\"{entity}/{project}/{_id}\")\n",
    "            runs.extend(list(sweep.runs))\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            run = api.run(f\"{entity}/{project}/{_id}\")\n",
    "            runs.append(run)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not load sweep or run '{_id}': {e}\")\n",
    "    return runs\n",
    "\n",
    "\n",
    "def _attn_cache_path(category_name, layer):\n",
    "    safe_cat = str(category_name).replace(\"/\", \"_\")\n",
    "    return PLOTTING_CACHE_DIR / f\"attn_{safe_cat}_layer{layer}.pkl\"\n",
    "\n",
    "\n",
    "def _load_attn_cache(category_name, layer):\n",
    "    path = _attn_cache_path(category_name, layer)\n",
    "    if path.exists():\n",
    "        try:\n",
    "            return pd.read_pickle(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to read attention cache {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _save_attn_cache(category_name, layer, df):\n",
    "    path = _attn_cache_path(category_name, layer)\n",
    "    try:\n",
    "        df.to_pickle(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: failed to write attention cache {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_attn_entropy_for_ids(ids, layer, entity=ENTITY, project=PROJECT):\n",
    "    \"\"\"Aggregate attn/layer{layer}/entropy/mean across runs.\"\"\"\n",
    "    metric_key = f\"attn/layer{layer}/entropy/mean\"\n",
    "    runs = _collect_runs_for_ids(ids, entity=entity, project=project)\n",
    "    records = []\n",
    "\n",
    "    for run in runs:\n",
    "        lr = None\n",
    "        try:\n",
    "            lr = run.config[\"optimizer_params\"][\"args\"][\"lr\"]\n",
    "        except Exception:\n",
    "            lr = run.config.get(\"optimizer_params.args.lr\", None)\n",
    "        if lr is None:\n",
    "            continue\n",
    "\n",
    "        for row in run.scan_history():\n",
    "            if metric_key not in row:\n",
    "                continue\n",
    "            val = row[metric_key]\n",
    "            step = row.get(\"_step\")\n",
    "            if step is None:\n",
    "                continue\n",
    "            if val is None:\n",
    "                continue\n",
    "            records.append({\"lr\": float(lr), \"step\": int(step), \"value\": float(val), \"run_id\": run.id})\n",
    "\n",
    "    if not records:\n",
    "        return pd.DataFrame(columns=[\"lr\", \"step\", \"mean\", \"std\", \"sem\", \"n\"])\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    grouped = (\n",
    "        df.groupby([\"lr\", \"step\"])[\"value\"].agg([\"mean\", \"std\", \"count\"]).reset_index().rename(columns={\"count\": \"n\"})\n",
    "    )\n",
    "    grouped[\"sem\"] = grouped[\"std\"] / np.sqrt(grouped[\"n\"].clip(lower=1))\n",
    "    grouped = grouped.sort_values([\"lr\", \"step\"])\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def plot_attn_entropy_category(\n",
    "    category_name,\n",
    "    ids,\n",
    "    layer,\n",
    "    ax=None,\n",
    "    ylim=DEFAULT_YLIM,\n",
    "    add_legend=True,\n",
    "    use_epoch_x=USE_EPOCH_X,\n",
    "    epoch_max=EPOCH_MAX,\n",
    "    use_cache=True,\n",
    "    lr_min=None,\n",
    "    lr_max=None,\n",
    "    print_end_stats=False,\n",
    "):\n",
    "    df = _load_attn_cache(category_name, layer) if use_cache else None\n",
    "    if df is None:\n",
    "        df = get_attn_entropy_for_ids(ids, layer)\n",
    "        if use_cache and not df.empty:\n",
    "            _save_attn_cache(category_name, layer, df)\n",
    "\n",
    "    if category_name == \"AdamW\" and not df.empty:\n",
    "        df = df[df[\"lr\"] <= 0.0024 + 1e-12]\n",
    "    if lr_min is not None:\n",
    "        df = df[df[\"lr\"] >= lr_min]\n",
    "    if lr_max is not None:\n",
    "        df = df[df[\"lr\"] <= lr_max]\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No data found for {category_name}, layer {layer} after LR filtering.\")\n",
    "        return\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    min_step = df[\"step\"].min() if not df.empty else 0\n",
    "    max_step = df[\"step\"].max() if not df.empty else 0\n",
    "    span = max(max_step - min_step, 1)\n",
    "\n",
    "    for lr, sub in df.groupby(\"lr\"):\n",
    "        sub = sub.sort_values(\"step\")\n",
    "        steps = sub[\"step\"].values.astype(float)\n",
    "        mean_vals = sub[\"mean\"].values\n",
    "        sem_vals = sub[\"sem\"].fillna(0.0).values\n",
    "\n",
    "        if use_epoch_x:\n",
    "            x_vals = (steps - min_step) / span * (epoch_max if epoch_max is not None else 1.0)\n",
    "        else:\n",
    "            x_vals = steps\n",
    "\n",
    "        ax.plot(x_vals, mean_vals, label=f\"{lr:g}\")\n",
    "        ax.fill_between(x_vals, mean_vals - sem_vals, mean_vals + sem_vals, alpha=0.25)\n",
    "\n",
    "        if print_end_stats and len(mean_vals) > 0:\n",
    "            final_mean = float(mean_vals[-1])\n",
    "            final_sem = float(sem_vals[-1])\n",
    "            lower = final_mean - final_sem\n",
    "            upper = final_mean + final_sem\n",
    "            print(\n",
    "                f\"{category_name}, layer {layer}, lr={lr:g}: \"\n",
    "                f\"final mean={final_mean:.4f}, lower={lower:.4f}, upper={upper:.4f}\"\n",
    "            )\n",
    "\n",
    "    title_base = ATTN_CATEGORY_TITLES.get(category_name, category_name)\n",
    "    ax.set_title(f\"Layer {layer} Attention Entropy for {title_base}\", pad=14)\n",
    "    ax.set_ylabel(\"Mean Entropy\")\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "\n",
    "    if use_epoch_x:\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        if epoch_max is not None:\n",
    "            ax.set_xlim(0.0, epoch_max)\n",
    "    else:\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        ax.set_xlim(left=0.0)\n",
    "\n",
    "    ax.tick_params(axis=\"y\", labelleft=True)\n",
    "    if add_legend:\n",
    "        ax.legend(title=\"Learning Rate\", fontsize=8)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_all_categories_combined(\n",
    "    categories_order=(\"AdamW\", \"Muon-NS-All\", \"Muon-PE-All\", \"Muon-NS-VOFFN\", \"Muon-PE-VOFFN\"),\n",
    "    layers=(0, 5, 11),\n",
    "    categories=ATTN_CATEGORIES,\n",
    "    ylim=DEFAULT_YLIM,\n",
    "    use_epoch_x=USE_EPOCH_X,\n",
    "    epoch_max=EPOCH_MAX,\n",
    "    use_cache=True,\n",
    "    lr_min=None,\n",
    "    lr_max=None,\n",
    "    lr_ranges_by_category=None,\n",
    "    base_lr_by_category=None,\n",
    "    print_end_stats=False,\n",
    "):\n",
    "    n_rows = len(layers)\n",
    "    n_cols = len(categories_order)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows), sharex=True, sharey=True)\n",
    "    if n_rows == 1:\n",
    "        axes = np.array([axes])\n",
    "    if n_cols == 1:\n",
    "        axes = axes.reshape(n_rows, 1)\n",
    "\n",
    "    for col, category_name in enumerate(categories_order):\n",
    "        if category_name not in categories:\n",
    "            print(f\"Warning: category {category_name} missing from ATTN_CATEGORIES; skipping column.\")\n",
    "            continue\n",
    "        ids = categories[category_name]\n",
    "        cat_lr_min, cat_lr_max = lr_min, lr_max\n",
    "        if lr_ranges_by_category and category_name in lr_ranges_by_category:\n",
    "            cat_lr_min, cat_lr_max = lr_ranges_by_category[category_name]\n",
    "\n",
    "        for row, layer in enumerate(layers):\n",
    "            ax = axes[row, col]\n",
    "            add_legend = row == 0\n",
    "            plot_attn_entropy_category(\n",
    "                category_name,\n",
    "                ids,\n",
    "                layer,\n",
    "                ax=ax,\n",
    "                ylim=ylim,\n",
    "                add_legend=add_legend,\n",
    "                use_epoch_x=use_epoch_x,\n",
    "                epoch_max=epoch_max,\n",
    "                use_cache=use_cache,\n",
    "                lr_min=cat_lr_min,\n",
    "                lr_max=cat_lr_max,\n",
    "                print_end_stats=print_end_stats,\n",
    "            )\n",
    "            if row > 0:\n",
    "                ax.set_xlabel(\"\")\n",
    "            if col > 0:\n",
    "                ax.set_ylabel(\"\")\n",
    "\n",
    "        top_ax = axes[0, col]\n",
    "        handles, labels = top_ax.get_legend_handles_labels()\n",
    "        if top_ax.get_legend() is not None:\n",
    "            top_ax.get_legend().remove()\n",
    "        if handles and labels:\n",
    "            fig.legend(\n",
    "                handles,\n",
    "                labels,\n",
    "                title=\"Learning Rate\",\n",
    "                loc=\"upper center\",\n",
    "                bbox_to_anchor=(0.5, COMBINED_LEGEND_Y),\n",
    "                ncol=min(4, len(labels)),\n",
    "                fontsize=8,\n",
    "            )\n",
    "        subtitle = ATTN_CATEGORY_TITLES.get(category_name, category_name)\n",
    "        x0, x1 = top_ax.get_position().x0, top_ax.get_position().x1\n",
    "        x_center = 0.5 * (x0 + x1) + COMBINED_SUBTITLE_X_NUDGE.get(category_name, 0)\n",
    "        fig.text(x_center, COMBINED_SUBTITLE_Y, subtitle, ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "    fig.suptitle(\"Attention Entropy vs Epoch\", y=COMBINED_TITLE_Y, fontsize=16, fontweight=\"bold\")\n",
    "    fig.tight_layout(rect=COMBINED_TIGHT_RECT)\n",
    "    plt.show()\n",
    "    return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06947d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combined attention entropy grid (per-category LR ranges)\n",
    "lr_ranges = {\n",
    "    \"AdamW\": (0.0003, 0.0024),\n",
    "    \"Muon-NS-All\": (0.01, 0.1),\n",
    "    \"Muon-PE-All\": (0.01, 0.1),\n",
    "    \"Muon-NS-VOFFN\": (0.003, 0.01),\n",
    "    \"Muon-PE-VOFFN\": (0.003, 0.01),\n",
    "}\n",
    "\n",
    "base_lrs = {\n",
    "    \"AdamW\": 0.0003,\n",
    "    \"Muon-NS-All\": 0.01,\n",
    "    \"Muon-PE-All\": 0.01,\n",
    "    \"Muon-NS-VOFFN\": 0.003,\n",
    "    \"Muon-PE-VOFFN\": 0.003,\n",
    "}\n",
    "\n",
    "fig, axes = plot_all_categories_combined(\n",
    "    lr_ranges_by_category=lr_ranges,\n",
    "    base_lr_by_category=base_lrs,\n",
    "    print_end_stats=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Benji branch plots: condition number, effective rank, spectral gap for Polar safety sweep\n",
    "\n",
    "def get_data(sweep_id, optimizer_param_name, optimizer_param_vals, update, layer, matrix, metric):\n",
    "    update_prefix = \"update_\" if update else \"\"\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"{ENTITY}/{PROJECT}/{sweep_id}\")\n",
    "    data = {val: [] for val in optimizer_param_vals}\n",
    "\n",
    "    for run in sweep.runs:\n",
    "        run_param = run.config[\"optimizer_params\"][\"args\"][optimizer_param_name]\n",
    "        data[run_param].append(\n",
    "            np.array([step.get(f\"svd/{update_prefix}layer{layer}_{matrix}/{metric}\") for step in run.scan_history()])\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_data(\n",
    "    sweep_id,\n",
    "    param_name,\n",
    "    param_id,\n",
    "    param_vals,\n",
    "    update,\n",
    "    layer,\n",
    "    matrix,\n",
    "    metric,\n",
    "    start,\n",
    "    end,\n",
    "    use_ci=False,\n",
    "    log_scale=False,\n",
    "    plot_med=True,\n",
    "    save_plot=False,\n",
    "):\n",
    "    print(\"Getting Data...\")\n",
    "    data = get_data(sweep_id, param_id, param_vals, update, layer, matrix, metric)\n",
    "    update_prefix = \"update_\" if update else \"\"\n",
    "\n",
    "    df = {}\n",
    "    for val in param_vals:\n",
    "        arrays = data[val]\n",
    "        valid_mask = arrays[0] != None\n",
    "        steps = np.where(valid_mask)[0]\n",
    "        valid_arrays = [x[valid_mask] for x in arrays]\n",
    "        valid_arrays = np.asarray(valid_arrays, dtype=float)\n",
    "\n",
    "        if use_ci:\n",
    "            med_vec = np.mean(np.stack(valid_arrays), axis=0)\n",
    "            std_vec = np.std(np.stack(valid_arrays), axis=0)\n",
    "            N = np.stack(valid_arrays).shape[0]\n",
    "            sem_vec = std_vec / np.sqrt(N)\n",
    "            min_vec = med_vec - 1.96 * sem_vec\n",
    "            max_vec = med_vec + 1.96 * sem_vec\n",
    "        else:\n",
    "            med_vec = np.median(np.stack(valid_arrays), axis=0)\n",
    "            min_vec = np.min(np.stack(valid_arrays), axis=0)\n",
    "            max_vec = np.max(np.stack(valid_arrays), axis=0)\n",
    "\n",
    "        med_vec = np.asarray(med_vec, dtype=float)\n",
    "        min_vec = np.asarray(min_vec, dtype=float)\n",
    "        max_vec = np.asarray(max_vec, dtype=float)\n",
    "\n",
    "        if log_scale:\n",
    "            med_vec = np.log(med_vec)\n",
    "            min_vec = np.log(min_vec)\n",
    "            max_vec = np.log(max_vec)\n",
    "\n",
    "        df[f\"med_vec_{val}\"] = med_vec\n",
    "        df[f\"min_vec_{val}\"] = min_vec\n",
    "        df[f\"max_vec_{val}\"] = max_vec\n",
    "\n",
    "    df[\"Step\"] = steps\n",
    "    df = pd.DataFrame(df)\n",
    "    df_iloc = df.iloc[start : end + 1]\n",
    "\n",
    "    print(\"Plotting Data...\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    for val in param_vals:\n",
    "        if plot_med:\n",
    "            sns.lineplot(data=df_iloc, x=\"Step\", y=f\"med_vec_{val}\")\n",
    "        steps = df_iloc[\"Step\"]\n",
    "        min_vec = df_iloc[f\"min_vec_{val}\"]\n",
    "        max_vec = df_iloc[f\"max_vec_{val}\"]\n",
    "        plt.fill_between(steps, min_vec, max_vec, alpha=0.3, label=f\"{val}\")\n",
    "\n",
    "    plt.title(f\"{update_prefix.replace('_', '').title()} Layer {layer} {matrix.title()} - {metric.replace('_', ' ').title()}\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    ylabel = f\"{metric.replace('_', ' ').title()} (Log Scale)\" if log_scale else metric.replace(\"_\", \" \").title()\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(title=param_name)\n",
    "\n",
    "    if save_plot:\n",
    "        Path(\"plotting/plots\").mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(f\"./plotting/plots/{param_id}_{metric}_layer{layer}.jpg\")\n",
    "        print(\"Plot Saved.\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb28ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reproduce Benji's polar_safety sweep plots\n",
    "POLAR_SWEEP_ID = \"5pllqyjx\"\n",
    "PARAM_NAME = \"Safety Factor\"\n",
    "PARAM_ID = \"polar_safety\"\n",
    "PARAM_VALS = [1, 1.01]\n",
    "\n",
    "# Condition number (layers 0,5,11), log-scale\n",
    "for layer in (0, 5, 11):\n",
    "    plot_data(\n",
    "        POLAR_SWEEP_ID,\n",
    "        PARAM_NAME,\n",
    "        PARAM_ID,\n",
    "        PARAM_VALS,\n",
    "        update=True,\n",
    "        layer=layer,\n",
    "        matrix=\"stacked\",\n",
    "        metric=\"condition_number\",\n",
    "        start=2,\n",
    "        end=32,\n",
    "        use_ci=True,\n",
    "        log_scale=True,\n",
    "        plot_med=False,\n",
    "        save_plot=True,\n",
    "    )\n",
    "\n",
    "# Effective rank (layers 0,5,11)\n",
    "for layer in (0, 5, 11):\n",
    "    plot_data(\n",
    "        POLAR_SWEEP_ID,\n",
    "        PARAM_NAME,\n",
    "        PARAM_ID,\n",
    "        PARAM_VALS,\n",
    "        update=True,\n",
    "        layer=layer,\n",
    "        matrix=\"stacked\",\n",
    "        metric=\"effective_rank\",\n",
    "        start=1,\n",
    "        end=32,\n",
    "        use_ci=True,\n",
    "        log_scale=False,\n",
    "        plot_med=False,\n",
    "        save_plot=True,\n",
    "    )\n",
    "\n",
    "# Spectral gap (layers 0,5,11)\n",
    "for layer in (0, 5, 11):\n",
    "    plot_data(\n",
    "        POLAR_SWEEP_ID,\n",
    "        PARAM_NAME,\n",
    "        PARAM_ID,\n",
    "        PARAM_VALS,\n",
    "        update=True,\n",
    "        layer=layer,\n",
    "        matrix=\"stacked\",\n",
    "        metric=\"spectral_gap\",\n",
    "        start=0,\n",
    "        end=32,\n",
    "        use_ci=False,\n",
    "        log_scale=False,\n",
    "        plot_med=False,\n",
    "        save_plot=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94725910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Variant-based plots for Muon/PE/AdamW sweeps\n",
    "VARIANT_SWEEP_IDS = [\"vqkcitxv\", \"ov703ihc\", \"js8l8c7m\"]\n",
    "VARIANT_TITLE_MAP = {\n",
    "    \"pe_all\": \"Muon+PE\",\n",
    "    \"ns_all\": \"Muon+NS\",\n",
    "    \"pe_voffn\": \"Muon+PE(VO/FFN)\",\n",
    "    \"ns_voffn\": \"Muon+NS(VO/FFN)\",\n",
    "    \"pe_mod_all\": \"Muon+PE (cheap)\",\n",
    "    \"adamw\": \"AdamW\",\n",
    "}\n",
    "CANONICAL_VARIANTS = [\n",
    "    \"AdamW\",\n",
    "    \"Muon+NS(VO/FFN)\",\n",
    "    \"Muon+NS\",\n",
    "    \"Muon+PE(VO/FFN)\",\n",
    "    \"Muon+PE\",\n",
    "    \"Muon+PE (cheap)\",\n",
    "    \"Muon+PE (split)\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc985c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _collect_variant_runs(sweep_ids=VARIANT_SWEEP_IDS, entity=ENTITY, project=PROJECT):\n",
    "    api = wandb.Api()\n",
    "    runs = []\n",
    "    for sid in sweep_ids:\n",
    "        try:\n",
    "            sweep = api.sweep(f\"{entity}/{project}/{sid}\")\n",
    "            runs.extend(list(sweep.runs))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not load sweep '{sid}': {e}\")\n",
    "    return runs\n",
    "\n",
    "\n",
    "def _get_variant_label(cfg):\n",
    "    raw_variant = None\n",
    "    muon_mode = None\n",
    "    try:\n",
    "        args = cfg[\"optimizer_params\"][\"args\"]\n",
    "        raw_variant = args.get(\"muon_variant\")\n",
    "        muon_mode = args.get(\"muon_mode\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    if raw_variant is None:\n",
    "        raw_variant = cfg.get(\"optimizer_params.args.muon_variant\")\n",
    "    if muon_mode is None:\n",
    "        muon_mode = cfg.get(\"optimizer_params.args.muon_mode\")\n",
    "    if raw_variant is None:\n",
    "        raw_variant = \"adamw\"\n",
    "    if muon_mode == \"split_qkv\":\n",
    "        return \"Muon+PE (split)\"\n",
    "    return VARIANT_TITLE_MAP.get(raw_variant, raw_variant)\n",
    "\n",
    "\n",
    "def _variant_cache_path(metric_key, sweep_ids):\n",
    "    safe_metric = metric_key.replace(\"/\", \"_\")\n",
    "    sid_part = \"_\".join(sorted(sweep_ids))\n",
    "    return PLOTTING_CACHE_DIR / f\"variants_{safe_metric}_{sid_part}.pkl\"\n",
    "\n",
    "\n",
    "def _load_variant_cache(metric_key, sweep_ids):\n",
    "    path = _variant_cache_path(metric_key, sweep_ids)\n",
    "    if path.exists():\n",
    "        try:\n",
    "            return pd.read_pickle(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to read variant cache {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _save_variant_cache(metric_key, sweep_ids, df):\n",
    "    path = _variant_cache_path(metric_key, sweep_ids)\n",
    "    try:\n",
    "        df.to_pickle(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: failed to write variant cache {path}: {e}\")\n",
    "\n",
    "\n",
    "def get_loss_by_variant(metric_key, sweep_ids=VARIANT_SWEEP_IDS, entity=ENTITY, project=PROJECT):\n",
    "    runs = _collect_variant_runs(sweep_ids=sweep_ids, entity=entity, project=project)\n",
    "    records = []\n",
    "\n",
    "    for run in runs:\n",
    "        cfg = run.config\n",
    "        variant = _get_variant_label(cfg)\n",
    "        for i, row in enumerate(run.scan_history()):\n",
    "            if metric_key not in row:\n",
    "                continue\n",
    "            val = row[metric_key]\n",
    "            step = row.get(\"_step\")\n",
    "            if step is None:\n",
    "                step = row.get(\"epoch\")\n",
    "            if step is None:\n",
    "                step = i\n",
    "            if val is None:\n",
    "                continue\n",
    "            records.append({\"variant\": variant, \"step\": float(step), \"value\": float(val), \"run_id\": run.id})\n",
    "\n",
    "    if not records:\n",
    "        print(f\"{metric_key}: no records found in sweeps {sweep_ids}\")\n",
    "        return pd.DataFrame(columns=[\"variant\", \"step\", \"mean\", \"std\", \"sem\", \"n\"])\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    grouped = (\n",
    "        df.groupby([\"variant\", \"step\"])[\"value\"].agg([\"mean\", \"std\", \"count\"]).reset_index().rename(columns={\"count\": \"n\"})\n",
    "    )\n",
    "    grouped[\"sem\"] = grouped[\"std\"] / np.sqrt(grouped[\"n\"].clip(lower=1))\n",
    "    grouped = grouped.sort_values([\"variant\", \"step\"])\n",
    "    print(f\"{metric_key}: found variants {sorted(grouped['variant'].unique())}\")\n",
    "    return grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e94fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_full_and_zoomed_variants(\n",
    "    sweep_ids=VARIANT_SWEEP_IDS,\n",
    "    zoom_window=(0.95, 1.0),\n",
    "    use_epoch_x=USE_EPOCH_X,\n",
    "    epoch_max=EPOCH_MAX,\n",
    "    use_cache=True,\n",
    "    include_split=True,\n",
    "):\n",
    "    if use_cache:\n",
    "        train_df_full = _load_variant_cache(\"train/loss\", sweep_ids)\n",
    "        val_df_full = _load_variant_cache(\"val/loss\", sweep_ids)\n",
    "        if train_df_full is None:\n",
    "            train_df_full = get_loss_by_variant(\"train/loss\", sweep_ids=sweep_ids)\n",
    "            _save_variant_cache(\"train/loss\", sweep_ids, train_df_full)\n",
    "        if val_df_full is None:\n",
    "            val_df_full = get_loss_by_variant(\"val/loss\", sweep_ids=sweep_ids)\n",
    "            _save_variant_cache(\"val/loss\", sweep_ids, val_df_full)\n",
    "    else:\n",
    "        train_df_full = get_loss_by_variant(\"train/loss\", sweep_ids=sweep_ids)\n",
    "        val_df_full = get_loss_by_variant(\"val/loss\", sweep_ids=sweep_ids)\n",
    "\n",
    "    if train_df_full.empty and val_df_full.empty:\n",
    "        print(\"No loss data found for the specified sweeps.\")\n",
    "        return\n",
    "\n",
    "    train_df_zoom = train_df_full[train_df_full[\"variant\"] != \"AdamW\"] if not train_df_full.empty else train_df_full\n",
    "    val_df_zoom = val_df_full[val_df_full[\"variant\"] != \"AdamW\"] if not val_df_full.empty else val_df_full\n",
    "\n",
    "    variants_full = set()\n",
    "    if not train_df_full.empty:\n",
    "        variants_full.update(train_df_full[\"variant\"].unique())\n",
    "    if not val_df_full.empty:\n",
    "        variants_full.update(val_df_full[\"variant\"].unique())\n",
    "    variants_full = [v for v in CANONICAL_VARIANTS if v in variants_full]\n",
    "\n",
    "    variants_zoom = set()\n",
    "    if not train_df_zoom.empty:\n",
    "        variants_zoom.update(train_df_zoom[\"variant\"].unique())\n",
    "    if not val_df_zoom.empty:\n",
    "        variants_zoom.update(val_df_zoom[\"variant\"].unique())\n",
    "    variants_zoom = [v for v in CANONICAL_VARIANTS if v in variants_zoom]\n",
    "\n",
    "    if not include_split:\n",
    "        variants_full = [v for v in variants_full if v != \"Muon+PE (split)\"]\n",
    "        variants_zoom = [v for v in variants_zoom if v != \"Muon+PE (split)\"]\n",
    "\n",
    "    if not variants_full:\n",
    "        print(\"No variants to plot after filtering.\")\n",
    "        return\n",
    "\n",
    "    palette = sns.color_palette(n_colors=len(CANONICAL_VARIANTS))\n",
    "    color_map = {name: palette[i] for i, name in enumerate(CANONICAL_VARIANTS)}\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    ax_train_full, ax_val_full = axes[0]\n",
    "    ax_train_zoom, ax_val_zoom = axes[1]\n",
    "\n",
    "    def _plot_full(df, ax, ylabel):\n",
    "        if df.empty:\n",
    "            ax.set_visible(False)\n",
    "            return\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        min_step = df[\"step\"].min() if not df.empty else 0.0\n",
    "        max_step = df[\"step\"].max() if not df.empty else 0.0\n",
    "        span = max(max_step - min_step, 1e-8)\n",
    "        for variant in variants_full:\n",
    "            sub = df[df[\"variant\"] == variant]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            sub = sub.sort_values(\"step\")\n",
    "            steps = sub[\"step\"].values.astype(float)\n",
    "            mean_vals = sub[\"mean\"].values\n",
    "            sem_vals = sub[\"sem\"].fillna(0.0).values\n",
    "            x_vals = (steps - min_step) / span * (epoch_max if epoch_max is not None else 1.0) if use_epoch_x else steps\n",
    "            color = color_map.get(variant)\n",
    "            ax.plot(x_vals, mean_vals, color=color, alpha=1.0, label=variant)\n",
    "            ax.fill_between(x_vals, mean_vals - sem_vals, mean_vals + sem_vals, alpha=0.2, color=color)\n",
    "        if use_epoch_x:\n",
    "            if epoch_max is not None:\n",
    "                ax.set_xlim(0.0, epoch_max)\n",
    "            ax.set_xlabel(\"Epoch\")\n",
    "        else:\n",
    "            ax.set_xlim(left=0.0)\n",
    "            ax.set_xlabel(\"Step\")\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.tick_params(axis=\"y\", labelleft=True)\n",
    "\n",
    "    def _plot_zoom(df, ax, ylabel):\n",
    "        if df.empty:\n",
    "            ax.set_visible(False)\n",
    "            return\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        emin, emax = zoom_window\n",
    "        min_step = df[\"step\"].min() if not df.empty else 0.0\n",
    "        max_step = df[\"step\"].max() if not df.empty else 0.0\n",
    "        span = max(max_step - min_step, 1e-8)\n",
    "        df_local = df.copy()\n",
    "        df_local[\"epoch\"] = (df_local[\"step\"] - min_step) / span * (epoch_max if epoch_max is not None else 1.0)\n",
    "        zoom_ymin, zoom_ymax = None, None\n",
    "        for variant in variants_zoom:\n",
    "            sub = df_local[df_local[\"variant\"] == variant]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            sub = sub.sort_values(\"step\")\n",
    "            x_vals = sub[\"epoch\"].values\n",
    "            mean_vals = sub[\"mean\"].values\n",
    "            sem_vals = sub[\"sem\"].fillna(0.0).values\n",
    "            color = color_map.get(variant)\n",
    "            ax.plot(x_vals, mean_vals, color=color, alpha=0.5)\n",
    "            mask = (x_vals >= emin) & (x_vals <= emax)\n",
    "            x_zoom = x_vals[mask]\n",
    "            mean_zoom = mean_vals[mask]\n",
    "            sem_zoom = sem_vals[mask]\n",
    "            boundary_x = []\n",
    "            boundary_mean = []\n",
    "            boundary_sem = []\n",
    "            if x_vals.size >= 2:\n",
    "                for b in (emin, emax):\n",
    "                    j = np.searchsorted(x_vals, b)\n",
    "                    if 0 < j < x_vals.size:\n",
    "                        x0, x1 = x_vals[j - 1], x_vals[j]\n",
    "                        if x1 == x0:\n",
    "                            continue\n",
    "                        t = (b - x0) / (x1 - x0)\n",
    "                        m = mean_vals[j - 1] + t * (mean_vals[j] - mean_vals[j - 1])\n",
    "                        s = sem_vals[j - 1] + t * (sem_vals[j] - sem_vals[j - 1])\n",
    "                        boundary_x.append(b)\n",
    "                        boundary_mean.append(m)\n",
    "                        boundary_sem.append(s)\n",
    "            if boundary_x:\n",
    "                x_ext = np.concatenate([x_zoom, np.array(boundary_x, dtype=float)])\n",
    "                mean_ext = np.concatenate([mean_zoom, np.array(boundary_mean, dtype=float)])\n",
    "                sem_ext = np.concatenate([sem_zoom, np.array(boundary_sem, dtype=float)])\n",
    "                order = np.argsort(x_ext)\n",
    "                x_zoom = x_ext[order]\n",
    "                mean_zoom = mean_ext[order]\n",
    "                sem_zoom = sem_ext[order]\n",
    "            if x_zoom.size == 0:\n",
    "                continue\n",
    "            ax.plot(x_zoom, mean_zoom, color=color, alpha=1.0)\n",
    "            ax.fill_between(x_zoom, mean_zoom - sem_zoom, mean_zoom + sem_zoom, alpha=0.2, color=color)\n",
    "            cur_min = float((mean_zoom - sem_zoom).min())\n",
    "            cur_max = float((mean_zoom + sem_zoom).max())\n",
    "            zoom_ymin = cur_min if zoom_ymin is None else min(zoom_ymin, cur_min)\n",
    "            zoom_ymax = cur_max if zoom_ymax is None else max(zoom_ymax, cur_max)\n",
    "        ax.set_xlim(emin, emax)\n",
    "        if zoom_ymin is not None and zoom_ymax is not None:\n",
    "            pad = max(0.05 * (zoom_ymax - zoom_ymin), 0.02)\n",
    "            ax.set_ylim(zoom_ymin - pad, zoom_ymax + pad)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.tick_params(axis=\"y\", labelleft=True)\n",
    "\n",
    "    _plot_full(train_df_full, ax_train_full, \"Train Loss\")\n",
    "    _plot_full(val_df_full, ax_val_full, \"Val Loss\")\n",
    "    _plot_zoom(train_df_zoom, ax_train_zoom, \"Train Loss\")\n",
    "    _plot_zoom(val_df_zoom, ax_val_zoom, \"Val Loss\")\n",
    "\n",
    "    xlabel = \"Epoch\" if use_epoch_x else \"Step\"\n",
    "    ax_train_zoom.set_xlabel(xlabel)\n",
    "    ax_val_zoom.set_xlabel(xlabel)\n",
    "\n",
    "    handles, labels = ax_train_full.get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        title=\"Optimizer Variant\",\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 0.97),\n",
    "        ncol=min(3, len(labels)),\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\"Train and Validation Loss by Optimizer Variant\", y=0.99, fontsize=16, fontweight=\"bold\")\n",
    "    fig.tight_layout(rect=(0, 0, 1, 0.93))\n",
    "    plt.show()\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def _temporary_variants(temp_variants):\n",
    "    global CANONICAL_VARIANTS\n",
    "    old_variants = CANONICAL_VARIANTS\n",
    "    try:\n",
    "        CANONICAL_VARIANTS = list(temp_variants)\n",
    "        yield\n",
    "    finally:\n",
    "        CANONICAL_VARIANTS = old_variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Default full + zoomed variants plot\n",
    "fig, axes = plot_full_and_zoomed_variants(\n",
    "    sweep_ids=VARIANT_SWEEP_IDS,\n",
    "    zoom_window=(0.95, 1.0),\n",
    "    use_epoch_x=True,\n",
    "    epoch_max=1.0,\n",
    "    use_cache=True,\n",
    "    include_split=True,\n",
    ")\n",
    "\n",
    "# Paper-specific subsets using temporary variant filtering\n",
    "_base_variants = [v for v in CANONICAL_VARIANTS if v not in (\"Muon+PE (cheap)\", \"Muon+PE (split)\")]\n",
    "with _temporary_variants(_base_variants):\n",
    "    plot_full_and_zoomed_variants(zoom_window=(0.95, 1.0), epoch_max=1.0, include_split=False)\n",
    "\n",
    "_pe_and_cheap = [\"Muon+PE\", \"Muon+PE (cheap)\"]\n",
    "with _temporary_variants(_pe_and_cheap):\n",
    "    plot_full_and_zoomed_variants(zoom_window=(0.95, 1.0), epoch_max=1.0, include_split=True)\n",
    "\n",
    "_pe_family = [\"Muon+PE\", \"Muon+PE(VO/FFN)\", \"Muon+PE (split)\"]\n",
    "with _temporary_variants(_pe_family):\n",
    "    plot_full_and_zoomed_variants(zoom_window=(0.95, 1.0), epoch_max=1.0, include_split=True)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}